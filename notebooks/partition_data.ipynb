{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9660ab1",
   "metadata": {},
   "source": [
    "## ğŸ§© Data Preprocessing for Ingestion Simulation\n",
    "\n",
    "In this stage, we preprocess the *All the News* dataset into structured, day-based partitions.  \n",
    "The goal is to simulate a **real-world data ingestion pipeline** for downstream document storage in **Qdrant** (for vector retrieval) and **Neo4j** (for graph relationships).\n",
    "\n",
    "### ğŸ¯ Objectives\n",
    "- **Normalize** textual data by removing excessive whitespace and line breaks to reduce storage bloat.  \n",
    "- **Parse and validate** publication dates into a consistent datetime format.  \n",
    "- **Partition** data efficiently by day to mimic incremental ingestion batches.  \n",
    "- **Prepare** clean CSV outputs for each day to serve as ingestion-ready artifacts.\n",
    "\n",
    "### âš™ï¸ Workflow\n",
    "1. **Load & filter** only the relevant columns:  \n",
    "   `date, year, month, day, author, title, article, url, section, publication`  \n",
    "2. **Sanitize** text columns (`article`, `title`, `author`) by collapsing multi-line text into single lines.  \n",
    "3. **Convert** date strings into proper datetime objects, extracting the date component.  \n",
    "4. **Group and write** records into per-day CSV partitions under the `data/` directory.\n",
    "\n",
    "Each output file represents a realistic **daily ingestion batch**, allowing us to later simulate:\n",
    "- Vector embedding creation and storage in **Qdrant**\n",
    "- Knowledge graph linking in **Neo4j**\n",
    "- Continuous **RAG pipeline evaluation**\n",
    "\n",
    "This modular preprocessing design mirrors how production pipelines would handle incoming text streams in real-world ML or data engineering systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15b10f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2688879, 10)\n",
      "shape: (5, 10)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ date          â”† year â”† month â”† day â”† â€¦ â”† article      â”† url          â”† section    â”† publication  â”‚\n",
      "â”‚ ---           â”† ---  â”† ---   â”† --- â”†   â”† ---          â”† ---          â”† ---        â”† ---          â”‚\n",
      "â”‚ datetime[Î¼s]  â”† i64  â”† f64   â”† i64 â”†   â”† str          â”† str          â”† str        â”† str          â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2016-12-09    â”† 2016 â”† 12.0  â”† 9   â”† â€¦ â”† This post is â”† https://www. â”† null       â”† Vox          â”‚\n",
      "â”‚ 18:31:00      â”†      â”†       â”†     â”†   â”† part of      â”† vox.com/poly â”†            â”†              â”‚\n",
      "â”‚               â”†      â”†       â”†     â”†   â”† Polyarchyâ€¦   â”† archy/â€¦      â”†            â”†              â”‚\n",
      "â”‚ 2016-10-07    â”† 2016 â”† 10.0  â”† 7   â”† â€¦ â”† The          â”† https://www. â”† null       â”† Business     â”‚\n",
      "â”‚ 21:26:46      â”†      â”†       â”†     â”†   â”† Indianapolis â”† businessinsi â”†            â”† Insider      â”‚\n",
      "â”‚               â”†      â”†       â”†     â”†   â”† Colts made   â”† der.coâ€¦      â”†            â”†              â”‚\n",
      "â”‚               â”†      â”†       â”†     â”†   â”† Aâ€¦           â”†              â”†            â”†              â”‚\n",
      "â”‚ 2018-01-26    â”† 2018 â”† 1.0   â”† 26  â”† â€¦ â”† DAVOS,       â”† https://www. â”† Davos      â”† Reuters      â”‚\n",
      "â”‚ 00:00:00      â”†      â”†       â”†     â”†   â”† Switzerland  â”† reuters.com/ â”†            â”†              â”‚\n",
      "â”‚               â”†      â”†       â”†     â”†   â”† (Reuters) -â€¦ â”† articlâ€¦      â”†            â”†              â”‚\n",
      "â”‚ 2019-06-27    â”† 2019 â”† 6.0   â”† 27  â”† â€¦ â”† PARIS        â”† https://www. â”† World News â”† Reuters      â”‚\n",
      "â”‚ 00:00:00      â”†      â”†       â”†     â”†   â”† (Reuters) -  â”† reuters.com/ â”†            â”†              â”‚\n",
      "â”‚               â”†      â”†       â”†     â”†   â”† Former       â”† articlâ€¦      â”†            â”†              â”‚\n",
      "â”‚               â”†      â”†       â”†     â”†   â”† Frencâ€¦       â”†              â”†            â”†              â”‚\n",
      "â”‚ 2016-01-27    â”† 2016 â”† 1.0   â”† 27  â”† â€¦ â”† Paris Hilton â”† https://www. â”† null       â”† TMZ          â”‚\n",
      "â”‚ 00:00:00      â”†      â”†       â”†     â”†   â”† arrived at   â”† tmz.com/2016 â”†            â”†              â”‚\n",
      "â”‚               â”†      â”†       â”†     â”†   â”† LAX Weâ€¦      â”† /01/27â€¦      â”†            â”†              â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "Error processing day None: 'NoneType' object has no attribute 'isoformat', chunk size: 1\n",
      "âœ… Total articles processed: 2688878\n",
      "âœ… Total days partitioned: 1555\n",
      "âœ… Partitioning 100.00% complete.\n",
      "ğŸ—‚ï¸ Partitioned files saved under: /Users/bing/Repositories/KG VectorDB RAG/data\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from kagglehub import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Setup paths ---\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"davidmckinley/all-the-news-dataset\")\n",
    "csv_file = Path(path) / \"all-the-news-2-1.csv\"\n",
    "\n",
    "# Resolve base paths\n",
    "base_dir = Path(__file__).resolve().parent if \"__file__\" in locals() else Path.cwd()\n",
    "project_root = base_dir.parent\n",
    "data_path = project_root / \"data\"\n",
    "\n",
    "data_path.mkdir(parents=True, exist_ok=True)  # ensure data directory exists\n",
    "\n",
    "use_columns = [\n",
    "    \"date\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"author\",\n",
    "    \"title\",\n",
    "    \"article\",\n",
    "    \"url\",\n",
    "    \"section\",\n",
    "    \"publication\"\n",
    "]\n",
    "# --- Load data ---\n",
    "df = pl.read_csv(\n",
    "    csv_file, \n",
    "    columns=use_columns,\n",
    "    infer_schema_length=100000,\n",
    "    ignore_errors=True,\n",
    "    try_parse_dates=True,\n",
    "    null_values=[\"\", \"NA\", \"NULL\"]\n",
    "    )\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df = df.with_columns(pl.col(\"date\").dt.date().alias(\"day\"))\n",
    "\n",
    "# --- Partition by day into data folder ---\n",
    "sum_articles = 0\n",
    "for day, daily_chunk in df.group_by(\"day\"):\n",
    "    try:\n",
    "        # each day gets its own subfolder inside data/\n",
    "        output_dir = data_path / f\"{day[0].isoformat()}\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        daily_chunk = daily_chunk.with_columns([\n",
    "            pl.col(\"article\").str.replace_all(r\"\\s+\", \" \").alias(\"article\"),\n",
    "            pl.col(\"title\").str.replace_all(r\"\\s+\", \" \").alias(\"title\"),\n",
    "            pl.col(\"author\").str.replace_all(r\"\\s+\", \" \").alias(\"author\"),\n",
    "        ])\n",
    "\n",
    "        # write the partition\n",
    "        daily_chunk.write_csv(output_dir / f\"news_articles_{len(daily_chunk)}.csv\")\n",
    "        sum_articles += len(daily_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing day {day[0]}: {e}, chunk size: {len(daily_chunk)}\")\n",
    "\n",
    "print(f\"âœ… Total articles processed: {sum_articles}\")\n",
    "print(f\"âœ… Total days partitioned: {len(df.select('day').unique())}\")\n",
    "print(f\"âœ… Partitioning {sum_articles/df.shape[0]:.2%} complete.\")\n",
    "print(f\"ğŸ—‚ï¸ Partitioned files saved under: {data_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dcba13",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
